{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "791e73e1",
   "metadata": {},
   "source": [
    "# Refactored BitrateLSTM Colab Notebook\n",
    "This notebook sets up the full data pipeline + 3-layer BitrateLSTM training on dummy data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4781904c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone your project repository\n",
    "!git clone https://github.com/YourUsername/split-inference-grpc-demo.git\n",
    "%cd split-inference-grpc-demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4e071b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Python dependencies\n",
    "!pip install -r requirements.txt\n",
    "# Ensure pyarrow/parquet support\n",
    "!pip install pandas pyarrow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ef84c6",
   "metadata": {},
   "source": [
    "## Generate Dummy Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fccf00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a dummy training dataset (parquet)\n",
    "!python scripts/prepare_data.py --dummy --output data/training_data.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aebbf42",
   "metadata": {},
   "source": [
    "## Load Dataset & DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b239d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from core.dataset import TraceDataset\n",
    "\n",
    "# Hyperparameters\n",
    "SEQ_LEN = 10\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "# Initialize dataset & loader\n",
    "dataset = TraceDataset(\n",
    "    parquet_path='data/training_data.parquet',\n",
    "    seq_len=SEQ_LEN,\n",
    "    normalize=True\n",
    ")\n",
    "loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "print(f'Dataset size: {len(dataset)}, Example X shape: {next(iter(loader))[0].shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9370c67",
   "metadata": {},
   "source": [
    "## Define & Instantiate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ff2efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.model import BitrateLSTM, quantile_loss\n",
    "\n",
    "# Model hyperparameters\n",
    "INPUT_SIZE = 1\n",
    "HIDDEN_SIZE = 128\n",
    "NUM_LAYERS = 3\n",
    "NUM_OUTPUTS = 3\n",
    "DROPOUT = 0.2\n",
    "LR = 1e-3\n",
    "\n",
    "# Build model\n",
    "model = BitrateLSTM(\n",
    "    input_size=INPUT_SIZE,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    num_outputs=NUM_OUTPUTS,\n",
    "    dropout=DROPOUT\n",
    ")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7598793e",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c5236f",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 3\n",
    "model.train()\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    total_loss = 0.0\n",
    "    for X_batch, y_batch in loader:\n",
    "        preds, _ = model(X_batch)  # (B, 3)\n",
    "        loss = quantile_loss(preds, y_batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * X_batch.size(0)\n",
    "    avg_loss = total_loss / len(dataset)\n",
    "    print(f'Epoch {epoch:02d}, Avg Loss: {avg_loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4fb9da",
   "metadata": {},
   "source": [
    "## (Optional) Save Model to Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0f18b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "# Save weights\n",
    "torch.save(model.state_dict(), '/content/drive/MyDrive/bitrate_model.pt')"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
